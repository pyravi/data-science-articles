{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caf560b-534f-46dd-8a13-1d047a9824db",
   "metadata": {},
   "source": [
    "# How to build your own Neural Network from scratch in Pytho\n",
    "A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence problems.\n",
    "\n",
    "![Create Neuron Network](https://miro.medium.com/max/1050/1*YMz-dKfX5JWSDjViYjxW8g.png)\n",
    "\n",
    "## In simple word, A neural newtwork is Mathematics function take an input and provide a desirable output by tuning weight and baise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aec6b9-c8f4-4d10-8549-729f65759ec6",
   "metadata": {},
   "source": [
    "![Prediction](https://miro.medium.com/max/1050/1*36MELEhgZsPFuzlZvObnxA.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ab14c7-63d8-4d37-8394-f93b723dae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x): return 1 / (1 + np.e**-x)\n",
    "\n",
    "def sigmoid_derivative(x): return x * (1.0 - x)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,x,y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input,self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1,self.weights2))\n",
    "        \n",
    "    def backprop(self):\n",
    "        \"Application of the change rule to find derivate of the loss functionwith respective to weights2 and weights1\"\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output)* sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y -self.output) * sigmoid_derivative(self.output),self.weights2.T)*sigmoid_derivative(self.layer1)))\n",
    "        #update the weigths with respective slope of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d972385-f50a-4800-af7a-482018f9c9b0",
   "metadata": {},
   "source": [
    "\n",
    "   ## We define 1 hidden layer and 1 output layer in this network.\n",
    "   \n",
    "   **Detail Summary** - Your input matrix X suggests that number of samples is 4 and the number of features is 3. The number of neurons in the input layer of a neural network is equal to the number of features*, not number of samples.\n",
    "   \n",
    "       For example, consider that you have 4 cars and you chose 3 features for each of them: color, number of seats and origin country.For each car sample, you feed these 3 features to the network and train your model. Even if you have 4000 samples, the number of input neurons do not change; it's 3. So self.weights1 is of shape (3, 4) where 3 is number of features and 4 is the number of hidden neurons (this 4 has nothing to do with the number of samples), as expected. Sometimes the inputs are augmented by 1 (or -1) to account for bias, so number of input neurons would be num_features + 1 in that case; but it's a choice of whether to deal with bias seperately or not.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b7bc96-6570-4547-96e8-692ce3a9dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00995226]\n",
      " [0.9709741 ]\n",
      " [0.9720075 ]\n",
      " [0.03520219]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0,0,1], [0,1,1],[1,0,1],[1,1,1]]) #Sample is 4 and feature is three\n",
    "    y = np.array([[0],[1],[1],[0]])\n",
    "    # print(X.shape,y.shape)# -> (4, 3) (4, 1)\n",
    "    # print(X.ndim, y.ndim)# -> 2 2\n",
    "\n",
    "    nn = NeuralNetwork(X,y)\n",
    "\n",
    "    for i in range(1500):\n",
    "        nn.feedforward()\n",
    "        nn.backprop()\n",
    "    print(nn.output)\n",
    "    # print(np.round(nn.output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709229c5-3e4d-4b94-8610-cf8e9122e873",
   "metadata": {},
   "source": [
    "# Each iteration of the training process consists of the following steps:\n",
    "\n",
    "Calculating the predicted output ŷ, known as feedforward\n",
    "* Updating the weights and biases, known as backpropagation\n",
    "* The sequential graph below illustrates the process.\n",
    "\n",
    "\n",
    "![sequential graph](https://miro.medium.com/max/1050/1*CEtt0h8Rss_qPu7CyqMTdQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f5ba6-edca-48e1-80f2-6a1f3ee3c7c5",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "There are many available loss functions, and the nature of our problem should dictate our choice of loss function. In this tutorial, we’ll use a simple sum-of-sqaures error as our loss function.\n",
    "\n",
    "![Error Calculation](https://miro.medium.com/max/450/1*iNa1VLdaeqwUAxpNXs3jwQ.png)\n",
    "## Our goal in training is to find the best set of weights and biases that minimizes the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ebf11-f87a-4e63-a347-5c6219ae98c0",
   "metadata": {},
   "source": [
    "![Loss function graph](https://miro.medium.com/max/1050/1*3FgDOt4kJxK2QZlb9T0cpg.png)\n",
    "If we have the derivative, we can simply update the weights and biases by increasing/reducing with it(refer to the diagram above). This is known as gradient descent.\n",
    "\n",
    "However, we can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.\n",
    "\n",
    "![Calculate](https://miro.medium.com/max/1050/1*7zxb2lfWWKaVxnmq2o69Mw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c2977-8427-4e7e-b91a-c622b6dfa76b",
   "metadata": {},
   "source": [
    "## Reference \n",
    "[30 High-Quality Data Science Articles Worth Reading](https://ai.plainenglish.io/top-30-high-quality-data-science-articles-worth-reading-bc02f38a29df)\n",
    "\n",
    "[How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)\n",
    "\n",
    "[Neural Networks from Scratch in Python -By Sentdex youtube series](https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3)\n",
    "\n",
    "[Neural Network -By 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d381bb2-1a16-4371-b478-2ab49e372979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
